{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "6446c4174f852fd8c2f4f989f4990ae150e615a8925597a8faa52fc3577391df"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Generative Adversarial Networks (GANs)\n",
    "GANs are a type of deep learning based generative models. Generative models are unsupervised learning method that try to find patterns in the input data in such a way that they can be used to generate new examples that pluasibly could have been drawm from the original dataset.\n",
    "\n",
    "GANs use two sub-models that a trained in a supervised method. The two sub-models are:\n",
    "* Generator model: It is used to generate a pluasible example from the input data.\n",
    "\n",
    "* Discriminator model: It is a classifier that will try to distingush between a generator generated example and the actual example from the input dataset. The generator tries to fool the Discriminator into classifying atleast more than half of the examples it generates as real examples.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import library"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "\n",
    "# Plotting libs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pytorch libs\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Image handling libs\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Torch version - 1.6.0 \nTorchvision version - 0.7.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Torch version - {torch.__version__} \\nTorchvision version - {torchvision.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale image values from -1 to 1 to be close to the output of the tanh function\n",
    "def scale(x, feature_range=(-1, 1)):\n",
    "    min, max = feature_range\n",
    "    x = x * (max-min) + min\n",
    "    return x"
   ]
  },
  {
   "source": [
    "### Discriminator\n",
    "A Discriminator is a NN with Linear layers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_size=28):\n",
    "        super().__init__()\n",
    "        input_features = 1 * image_size * image_size\n",
    "        self.hcl1 = nn.Linear(input_features, 1024)\n",
    "        self.hcl2 = nn.Linear(1024, 512)\n",
    "        self.hcl3 = nn.Linear(512, 256)\n",
    "        self.out = nn.Linear(256, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.hcl1(x))\n",
    "        x = self.leaky_relu(self.hcl2(x))\n",
    "        x = self.leaky_relu(self.hcl3(x))\n",
    "        x = torch.sigmoid(self.out(x))\n",
    "        return x"
   ]
  },
  {
   "source": [
    "### Generator"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_block(in_features, out_features, batch_norm=True):\n",
    "    layers = []\n",
    "    if batch_norm:\n",
    "        linear_layer = nn.Linear(in_features, out_features, bias=False)\n",
    "        batch_norm = nn.BatchNorm1d(out_features)\n",
    "        layers = [linear_layer, batch_norm]\n",
    "    else:\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_features, image_size=28):\n",
    "        super().__init__()\n",
    "        output_features = 1 * image_size * image_size\n",
    "        self.hcl1 = linear_block(input_features, 256)\n",
    "        self.hcl2 = linear_block(256, 512)\n",
    "        self.hcl3 = linear_block(512, 1024)\n",
    "        self.output = linear_block(1024, output_features)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.hcl1(x))\n",
    "        x = self.leaky_relu(self.hcl2(x))\n",
    "        x = self.leaky_relu(self.hcl3(x))\n",
    "        x = torch.tanh(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "source": [
    "### Auxiliary functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_vector(batch_size, length):\n",
    "    # Sample from a Gaussian distribution\n",
    "    z_vec = torch.randn(batch_size, length).float()\n",
    "    if torch.cuda.is_available():\n",
    "        z_vec = z_vec.cuda()\n",
    "    return z_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_loss(loss_dict, save=False, path=\"./loss.png\"):\n",
    "    x = range(len(loss_dict['D_loss']))\n",
    "    \n",
    "    generator_loss = loss_dict['G_loss']\n",
    "    discriminator_loss = loss_dict['D_loss']\n",
    "\n",
    "    plt.plot(x, generator_loss, label='Generator loss')\n",
    "    plt.plot(x, discriminator_loss, label='Discriminator loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend(loc=4) # 4 = Lower right\n",
    "    plt.grid(True) # Show grid\n",
    "    plt.tight_layout() # Crop so no extra white margin\n",
    "\n",
    "    if (save):\n",
    "        plt.savefig(path)\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(num_epoch, display=False, save=False, path='./result.png', fixed_noise=None):\n",
    "    random_noise = random_vector(5*5, 100)\n",
    "    random_noise = Variable(random_noise.cuda(), volatile=True)\n",
    "\n",
    "    G.eval() # We do not want to backprop while evaluation\n",
    "    if (fixed_noise != None):\n",
    "        test_imgs = G(fixed_noise)\n",
    "    else:\n",
    "        test_imgs = G(random_noise)\n",
    "    G.train()\n",
    "\n",
    "    fig, ax = plt.subplots(5, 5, figsize=(5, 5))\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            ax[i, j].get_xaxis().set_visible(False)\n",
    "            ax[i, j].get_yaxis().set_visible(False)\n",
    "    \n",
    "    for k in range(5*5):\n",
    "        i = int(k / 5)\n",
    "        j = int(k % 5)\n",
    "        ax[i, j].cla() # Clear the current axes\n",
    "        ax[i, j].imshow(test_imgs[k, :].cpu().data.view(28, 28).numpy(), cmap='gray')\n",
    "\n",
    "    fig.text(0.5, 0.04, f'Epoch {num_epoch}', ha='center')\n",
    "    if (save):\n",
    "        fig.savefig(path)\n",
    "    \n",
    "    if (display):\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "source": [
    "### Define constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE =  0.0002\n",
    "EPOCHS = 100\n",
    "z_size = 100\n",
    "FIXED_NOISE = Variable(torch.randn((5*5, 100)).cuda(), volatile=True)"
   ]
  },
  {
   "source": [
    "### Import data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.5,), \n",
    "            std = (0.5,)\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        'data', \n",
    "        train=True, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "source": [
    "### Define Generator and Discriminator"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (hcl1): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=256, bias=False)\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (hcl2): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=512, bias=False)\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (hcl3): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (output): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=784, bias=False)\n",
       "    (1): BatchNorm1d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "# Generator will take Random noise of size 100 and output a 28*28 image\n",
    "G = Generator(input_features=z_size)\n",
    "G.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (hcl1): Linear(in_features=784, out_features=1024, bias=True)\n",
       "  (hcl2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (hcl3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (out): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "# Discriminator will take in 28*28 image and output a real/fake\n",
    "D = Discriminator()\n",
    "D.cuda()"
   ]
  },
  {
   "source": [
    "### Define Loss and Optimizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross Entropy loss\n",
    "LOSS = nn.BCELoss()\n",
    "\n",
    "# Optimizer\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=LEARNING_RATE)\n",
    "# LR Schedular\n",
    "G_schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(G_optimizer, mode='min',factor=0.01, patience=2, threshold=1e-2, verbose=True)\n",
    "\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=LEARNING_RATE)\n",
    "# LR Schedular\n",
    "D_schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(D_optimizer, mode='min',factor=0.01, patience=2, threshold=1e-2, verbose=True)"
   ]
  },
  {
   "source": [
    "### Create output files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results save folder\n",
    "if not os.path.isdir('Results'):\n",
    "    os.mkdir('Results')\n",
    "if not os.path.isdir('Results/Random'):\n",
    "    os.mkdir('Results/Random')\n",
    "if not os.path.isdir('Results/Fixed'):\n",
    "    os.mkdir('Results/Fixed')"
   ]
  },
  {
   "source": [
    "### Training loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_loss(predictions):\n",
    "    batch_size = predictions.shape[0]\n",
    "    labels = torch.ones(batch_size)\n",
    "    # We use the binary cross entropy loss | Model has a sigmoid function\n",
    "    criterion = nn.BCELoss()\n",
    "    # Move models to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        labels = labels.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    loss = criterion(predictions.squeeze(), labels)\n",
    "    return loss\n",
    "\n",
    "def fake_loss(predictions):\n",
    "    batch_size = predictions.shape[0]\n",
    "    labels = torch.zeros(batch_size)\n",
    "    criterion = nn.BCELoss()\n",
    "    # Move models to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        labels = labels.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    loss = criterion(predictions.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(generator, discriminator, optimizer, real_data, batch_size, z_size):\n",
    "    # Reshape real_data to vector\n",
    "    real_data = real_data.view(batch_size, -1)\n",
    "    # Rescale real_data to range -1 - 1\n",
    "    real_data = scale(real_data)\n",
    "    \n",
    "    # Reset gradients and set model to training mode\n",
    "    optimizer.zero_grad()\n",
    "    discriminator.train()\n",
    "    \n",
    "    # Train on real data\n",
    "    real_data_logits = discriminator.forward(real_data)\n",
    "    loss_real = real_loss(real_data_logits)\n",
    "    # Generate fake data\n",
    "    z_vec = random_vector(batch_size, z_size)\n",
    "    fake_data = generator.forward(z_vec)\n",
    "    # Train on fake data\n",
    "    fake_data_logits = discriminator.forward(fake_data)\n",
    "    loss_fake = fake_loss(fake_data_logits)\n",
    "    # Calculate total loss\n",
    "    total_loss = loss_real + loss_fake\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(generator, discriminator, optimizer, batch_size, z_size):\n",
    "    # Reset gradients and set model to training mode\n",
    "    optimizer.zero_grad()\n",
    "    generator.train()\n",
    "    # Generate fake data\n",
    "    z_vec = random_vector(batch_size, z_size)\n",
    "    fake_data = generator.forward(z_vec)\n",
    "    # Train generator with output of discriminator\n",
    "    discriminator_logits = discriminator.forward(fake_data)\n",
    "    # Reverse labels\n",
    "    loss = real_loss(discriminator_logits)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1  Generator loss 8.061975479125977 ; Descriminator loss 0.05156688764691353: 100%|██████████| 469/469 [00:31<00:00, 15.11it/s]\n",
      "Epoch 2  Generator loss 6.027048110961914 ; Descriminator loss 0.0957651138305664: 100%|██████████| 469/469 [00:45<00:00, 10.25it/s]\n",
      "Epoch 3  Generator loss 7.420546054840088 ; Descriminator loss 0.0007998064975254238: 100%|██████████| 469/469 [00:31<00:00, 15.10it/s]\n",
      "Epoch 4  Generator loss 8.755077362060547 ; Descriminator loss 0.00018719909712672234: 100%|██████████| 469/469 [00:27<00:00, 16.98it/s]\n",
      "Epoch 5  Generator loss 9.476887702941895 ; Descriminator loss 9.070510714082047e-05: 100%|██████████| 469/469 [00:39<00:00, 11.83it/s]\n",
      "Epoch     5: reducing learning rate of group 0 to 2.0000e-06.\n",
      "Epoch 6  Generator loss 10.121030807495117 ; Descriminator loss 4.91005448566284e-05: 100%|██████████| 469/469 [00:43<00:00, 10.86it/s]\n",
      "Epoch 7  Generator loss 10.763690948486328 ; Descriminator loss 2.5618632207624614e-05: 100%|██████████| 469/469 [00:39<00:00, 11.76it/s]\n",
      "Epoch 8  Generator loss 11.219356536865234 ; Descriminator loss 1.6213642084039748e-05: 100%|██████████| 469/469 [00:40<00:00, 11.56it/s]\n",
      "Epoch     8: reducing learning rate of group 0 to 2.0000e-08.\n",
      "Epoch 9  Generator loss 11.622941017150879 ; Descriminator loss 1.0835604371095542e-05: 100%|██████████| 469/469 [00:40<00:00, 11.67it/s]\n",
      "Epoch 10  Generator loss 12.019619941711426 ; Descriminator loss 7.3238666118413676e-06: 100%|██████████| 469/469 [00:40<00:00, 11.64it/s]\n",
      "Epoch 11  Generator loss 12.400447845458984 ; Descriminator loss 4.998575604986399e-06: 100%|██████████| 469/469 [00:40<00:00, 11.72it/s]\n",
      "Epoch    11: reducing learning rate of group 0 to 2.0000e-10.\n",
      "Epoch 12  Generator loss 12.772229194641113 ; Descriminator loss 3.42814382747747e-06: 100%|██████████| 469/469 [00:37<00:00, 12.55it/s]\n",
      "Epoch 13  Generator loss 13.140109062194824 ; Descriminator loss 2.373497636654065e-06: 100%|██████████| 469/469 [00:34<00:00, 13.61it/s]\n",
      "Epoch 14  Generator loss 13.495255470275879 ; Descriminator loss 1.6666120927766315e-06: 100%|██████████| 469/469 [00:32<00:00, 14.26it/s]\n",
      "Epoch 15  Generator loss 13.830495834350586 ; Descriminator loss 1.1863439794979058e-06: 100%|██████████| 469/469 [00:36<00:00, 12.87it/s]\n",
      "Epoch 16  Generator loss 14.162894248962402 ; Descriminator loss 8.559892989978835e-07: 100%|██████████| 469/469 [00:30<00:00, 15.60it/s]\n",
      "Epoch 17  Generator loss 14.480661392211914 ; Descriminator loss 6.212217726897507e-07: 100%|██████████| 469/469 [00:33<00:00, 13.93it/s]\n",
      "Epoch 18  Generator loss 14.785250663757324 ; Descriminator loss 4.5637952439392393e-07: 100%|██████████| 469/469 [00:28<00:00, 16.64it/s]\n",
      "Epoch 19  Generator loss 15.093537330627441 ; Descriminator loss 3.3601452287257416e-07: 100%|██████████| 469/469 [00:29<00:00, 15.73it/s]\n",
      "Epoch 20  Generator loss 15.395791053771973 ; Descriminator loss 2.493058559593919e-07: 100%|██████████| 469/469 [00:27<00:00, 17.08it/s]\n",
      "Epoch 21  Generator loss 15.692639350891113 ; Descriminator loss 1.8586042926926893e-07: 100%|██████████| 469/469 [00:31<00:00, 14.93it/s]\n",
      "Epoch 22  Generator loss 15.985795021057129 ; Descriminator loss 1.3902190687531402e-07: 100%|██████████| 469/469 [00:27<00:00, 17.08it/s]\n",
      "Epoch 23  Generator loss 16.279300689697266 ; Descriminator loss 1.0231178038111466e-07: 100%|██████████| 469/469 [00:31<00:00, 15.04it/s]\n",
      "Epoch 24  Generator loss 16.558454513549805 ; Descriminator loss 8.023042852300932e-08: 100%|██████████| 469/469 [00:28<00:00, 16.26it/s]\n",
      "Epoch 25  Generator loss 16.849855422973633 ; Descriminator loss 6.503173466398948e-08: 100%|██████████| 469/469 [00:29<00:00, 15.71it/s]\n",
      "Epoch 26  Generator loss 17.13039779663086 ; Descriminator loss 4.852177681868852e-08: 100%|██████████| 469/469 [00:31<00:00, 14.92it/s]\n",
      "Epoch 27  Generator loss 17.411367416381836 ; Descriminator loss 2.872372206752516e-08: 100%|██████████| 469/469 [00:32<00:00, 14.44it/s]\n",
      "Epoch 28  Generator loss 17.692697525024414 ; Descriminator loss 1.3119601405264802e-08: 100%|██████████| 469/469 [00:30<00:00, 15.20it/s]\n",
      "Epoch 29  Generator loss 17.97088050842285 ; Descriminator loss 5.325153207280664e-09: 100%|██████████| 469/469 [00:35<00:00, 13.29it/s]\n",
      "Epoch 30  Generator loss 18.24576759338379 ; Descriminator loss 2.4901463113735645e-09: 100%|██████████| 469/469 [00:30<00:00, 15.22it/s]\n",
      "Epoch 31  Generator loss 18.5184383392334 ; Descriminator loss 1.5558450083830166e-09: 100%|██████████| 469/469 [00:30<00:00, 15.22it/s]\n",
      "Epoch 32  Generator loss 18.79296875 ; Descriminator loss 1.179543129836702e-09: 100%|██████████| 469/469 [00:32<00:00, 14.24it/s]\n",
      "Epoch 33  Generator loss 19.052406311035156 ; Descriminator loss 8.816786190024573e-10: 100%|██████████| 469/469 [00:32<00:00, 14.44it/s]\n",
      "Epoch    33: reducing learning rate of group 0 to 2.0000e-06.\n",
      "Epoch 34  Generator loss 19.200786590576172 ; Descriminator loss 9.3529417544147e-10: 100%|██████████| 469/469 [00:31<00:00, 15.09it/s]\n",
      "Epoch 35  Generator loss 19.195201873779297 ; Descriminator loss 8.518921124078815e-10: 100%|██████████| 469/469 [00:32<00:00, 14.45it/s]\n",
      "Epoch 36  Generator loss 19.19860076904297 ; Descriminator loss 7.90995491328772e-10: 100%|██████████| 469/469 [00:30<00:00, 15.20it/s]\n",
      "Epoch    36: reducing learning rate of group 0 to 2.0000e-08.\n",
      "Epoch 37  Generator loss 19.187402725219727 ; Descriminator loss 7.790809108954022e-10: 100%|██████████| 469/469 [00:30<00:00, 15.16it/s]\n",
      "Epoch 38  Generator loss 19.1959228515625 ; Descriminator loss 7.784189959281207e-10: 100%|██████████| 469/469 [00:30<00:00, 15.18it/s]\n",
      "Epoch 39  Generator loss 19.20598030090332 ; Descriminator loss 7.784189404169695e-10: 100%|██████████| 469/469 [00:33<00:00, 14.04it/s]\n",
      "Epoch    39: reducing learning rate of group 0 to 2.0000e-10.\n",
      "Epoch 40  Generator loss 19.19798469543457 ; Descriminator loss 7.790809108954022e-10: 100%|██████████| 469/469 [00:31<00:00, 15.07it/s]\n",
      "Epoch 41  Generator loss 19.197999954223633 ; Descriminator loss 7.784189959281207e-10: 100%|██████████| 469/469 [00:31<00:00, 15.04it/s]\n",
      "Epoch 42  Generator loss 19.202533721923828 ; Descriminator loss 7.784189959281207e-10: 100%|██████████| 469/469 [00:31<00:00, 14.75it/s]\n",
      "Epoch 43  Generator loss 19.197328567504883 ; Descriminator loss 7.784189959281207e-10: 100%|██████████| 469/469 [00:29<00:00, 16.16it/s]\n",
      "Epoch 44  Generator loss 19.197093963623047 ; Descriminator loss 7.784189959281207e-10: 100%|██████████| 469/469 [00:30<00:00, 15.32it/s]\n",
      "Epoch 45  Generator loss 19.198606491088867 ; Descriminator loss 7.790809108954022e-10: 100%|██████████| 469/469 [00:27<00:00, 17.23it/s]\n",
      "Epoch 46  Generator loss 19.201406478881836 ; Descriminator loss 7.784189959281207e-10: 100%|██████████| 469/469 [00:28<00:00, 16.40it/s]\n",
      "Epoch 47  Generator loss 19.20853042602539 ; Descriminator loss 7.784189404169695e-10: 100%|██████████| 469/469 [00:29<00:00, 16.11it/s]\n",
      "Epoch 48  Generator loss 19.189945220947266 ; Descriminator loss 7.784189959281207e-10: 100%|██████████| 469/469 [00:29<00:00, 15.74it/s]\n",
      "Epoch 49  Generator loss 19.207748413085938 ; Descriminator loss 7.790809108954022e-10: 100%|██████████| 469/469 [00:28<00:00, 16.42it/s]\n",
      "Epoch 50  Generator loss 19.19415283203125 ; Descriminator loss 8.26875234949398e-10:  23%|██▎       | 106/469 [00:04<00:14, 25.33it/s] \n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-46edcb03e914>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;31m# Display Epoch and loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mdisplay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'Epoch {epoch + 1}  Generator loss {torch.mean(torch.FloatTensor(G_loss))} ; Descriminator loss {torch.mean(torch.FloatTensor(D_loss))}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mtrainProgressBar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Call lr_schedular\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mset_description\u001b[1;34m(self, desc, refresh)\u001b[0m\n\u001b[0;32m   1376\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdesc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': '\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdesc\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1378\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_description_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mrefresh\u001b[1;34m(self, nolock, lock_args)\u001b[0m\n\u001b[0;32m   1337\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1339\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1340\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1341\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mdisplay\u001b[1;34m(self, msg, pos)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mprint_status\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[0mfp_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mfp_write\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m             \u001b[0mfp_flush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_dict = {}\n",
    "loss_dict['D_loss'] = []\n",
    "loss_dict['G_loss'] = []\n",
    "for epoch in range(EPOCHS):\n",
    "    # Local losses\n",
    "    D_loss = []\n",
    "    G_loss = []\n",
    "    # Training progress bar\n",
    "    trainProgressBar = tqdm(train_loader)\n",
    "\n",
    "    for X in trainProgressBar:\n",
    "        X = X[0]\n",
    "        if torch.cuda.is_available():\n",
    "            X = X.cuda()\n",
    "\n",
    "        # Train discriminator D\n",
    "        d_loss = train_discriminator(G, D, D_optimizer, X, X.shape[0], z_size)\n",
    "        D_loss.append(d_loss)\n",
    "\n",
    "        # [==========================xxx============================]\n",
    "        \n",
    "        # Train generator G\n",
    "        g_loss = train_generator(G, D, G_optimizer, X.shape[0], z_size)\n",
    "        G_loss.append(g_loss)\n",
    "        \n",
    "        # [==========================xxx============================]\n",
    "            # Display Epoch and loss\n",
    "        display = f'Epoch {epoch + 1}  Generator loss {torch.mean(torch.FloatTensor(G_loss))} ; Descriminator loss {torch.mean(torch.FloatTensor(D_loss))}'\n",
    "        trainProgressBar.set_description(display)\n",
    "\n",
    "    # Call lr_schedular\n",
    "    G_schedular.step(G_loss[-1])\n",
    "    D_schedular.step(D_loss[-1])\n",
    "\n",
    "    # Adding loss of this epoch to the global loss dict\n",
    "    loss_dict['D_loss'].append(torch.mean(torch.FloatTensor(D_loss)))\n",
    "    loss_dict['G_loss'].append(torch.mean(torch.FloatTensor(G_loss)))\n",
    "    # Saving the results of this epoch\n",
    "    show_result(epoch + 1, save=True, path='Results/Random/' + str(epoch + 1) + '.png')\n",
    "    show_result(epoch + 1, save=True, path='Results/Fixed/' + str(epoch + 1) + '.png', fixed_noise=FIXED_NOISE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_loss(loss_dict, save=True, path='./GAN_graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for e in range(EPOCHS):\n",
    "    img_name = 'Results/Fixed/' + str(e + 1) + '.png'\n",
    "    images.append(imageio.imread(img_name))\n",
    "imageio.mimsave('Results/generation_animation.gif', images, fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}