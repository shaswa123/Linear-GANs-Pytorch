{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "6446c4174f852fd8c2f4f989f4990ae150e615a8925597a8faa52fc3577391df"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Generative Adversarial Networks (GANs)\n",
    "GANs are a type of deep learning based generative models. Generative models are unsupervised learning method that try to find patterns in the input data in such a way that they can be used to generate new examples that pluasibly could have been drawm from the original dataset.\n",
    "\n",
    "GANs use two sub-models that a trained in a supervised method. The two sub-models are:\n",
    "* Generator model: It is used to generate a pluasible example from the input data.\n",
    "\n",
    "* Discriminator model: It is a classifier that will try to distingush between a generator generated example and the actual example from the input dataset. The generator tries to fool the Discriminator into classifying atleast more than half of the examples it generates as real examples.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import library"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "\n",
    "# Plotting libs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pytorch libs\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Image handling libs\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Torch version - {torch.__version__} \\nTorchvision version - {torchvision.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale image values from -1 to 1 to be close to the output of the tanh function\n",
    "def scale(x, feature_range=(-1, 1)):\n",
    "    min, max = feature_range\n",
    "    x = x * (max-min) + min\n",
    "    return x"
   ]
  },
  {
   "source": [
    "### Discriminator\n",
    "A Discriminator is a NN with Linear layers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_size=28):\n",
    "        super().__init__()\n",
    "        input_features = 1 * image_size * image_size\n",
    "        self.hcl1 = nn.Linear(input_features, 1024)\n",
    "        self.hcl2 = nn.Linear(1024, 512)\n",
    "        self.hcl3 = nn.Linear(512, 256)\n",
    "        self.out = nn.Linear(256, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.hcl1(x))\n",
    "        x = self.leaky_relu(self.hcl2(x))\n",
    "        x = self.leaky_relu(self.hcl3(x))\n",
    "        x = torch.sigmoid(self.out(x))\n",
    "        return x"
   ]
  },
  {
   "source": [
    "### Generator"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_block(in_features, out_features, batch_norm=True):\n",
    "    layers = []\n",
    "    if batch_norm:\n",
    "        linear_layer = nn.Linear(in_features, out_features, bias=False)\n",
    "        batch_norm = nn.BatchNorm1d(out_features)\n",
    "        layers = [linear_layer, batch_norm]\n",
    "    else:\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_features, image_size=28):\n",
    "        super().__init__()\n",
    "        output_features = 1 * image_size * image_size\n",
    "        self.hcl1 = linear_block(input_features, 256)\n",
    "        self.hcl2 = linear_block(256, 512)\n",
    "        self.hcl3 = linear_block(512, 1024)\n",
    "        self.output = linear_block(1024, output_features)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.hcl1(x))\n",
    "        x = self.leaky_relu(self.hcl2(x))\n",
    "        x = self.leaky_relu(self.hcl3(x))\n",
    "        x = torch.tanh(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "source": [
    "### Auxiliary functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_vector(batch_size, length):\n",
    "    # Sample from a Gaussian distribution\n",
    "    z_vec = torch.randn(batch_size, length).float()\n",
    "    if torch.cuda.is_available():\n",
    "        z_vec = z_vec.cuda()\n",
    "    return z_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_loss(loss_dict, save=False, path=\"./loss.png\"):\n",
    "    x = range(len(loss_dict['D_loss']))\n",
    "    \n",
    "    generator_loss = loss_dict['G_loss']\n",
    "    discriminator_loss = loss_dict['D_loss']\n",
    "\n",
    "    plt.plot(x, generator_loss, label='Generator loss')\n",
    "    plt.plot(x, discriminator_loss, label='Discriminator loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend(loc=4) # 4 = Lower right\n",
    "    plt.grid(True) # Show grid\n",
    "    plt.tight_layout() # Crop so no extra white margin\n",
    "\n",
    "    if (save):\n",
    "        plt.savefig(path)\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(num_epoch, display=False, save=False, path='./result.png', fixed_noise=None):\n",
    "    random_noise = random_vector(5*5, 100)\n",
    "    random_noise = Variable(random_noise.cuda(), volatile=True)\n",
    "\n",
    "    G.eval() # We do not want to backprop while evaluation\n",
    "    if (fixed_noise != None):\n",
    "        test_imgs = G(fixed_noise)\n",
    "    else:\n",
    "        test_imgs = G(random_noise)\n",
    "    G.train()\n",
    "\n",
    "    fig, ax = plt.subplots(5, 5, figsize=(5, 5))\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            ax[i, j].get_xaxis().set_visible(False)\n",
    "            ax[i, j].get_yaxis().set_visible(False)\n",
    "    \n",
    "    for k in range(5*5):\n",
    "        i = int(k / 5)\n",
    "        j = int(k % 5)\n",
    "        ax[i, j].cla() # Clear the current axes\n",
    "        ax[i, j].imshow(test_imgs[k, :].cpu().data.view(28, 28).numpy(), cmap='gray')\n",
    "\n",
    "    fig.text(0.5, 0.04, f'Epoch {num_epoch}', ha='center')\n",
    "    if (save):\n",
    "        fig.savefig(path)\n",
    "    \n",
    "    if (display):\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "source": [
    "### Define constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE =  0.0002\n",
    "EPOCHS = 100\n",
    "z_size = 100\n",
    "FIXED_NOISE = Variable(torch.randn((5*5, 100)).cuda(), volatile=True)"
   ]
  },
  {
   "source": [
    "### Import data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.5,), \n",
    "            std = (0.5,)\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        'data', \n",
    "        train=True, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "source": [
    "### Define Generator and Discriminator"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator will take Random noise of size 100 and output a 28*28 image\n",
    "G = Generator(input_features=z_size)\n",
    "G.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator will take in 28*28 image and output a real/fake\n",
    "D = Discriminator()\n",
    "D.cuda()"
   ]
  },
  {
   "source": [
    "### Define Loss and Optimizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross Entropy loss\n",
    "LOSS = nn.BCELoss()\n",
    "\n",
    "# Optimizer\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=LEARNING_RATE )\n",
    "# LR Schedular\n",
    "G_schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(G_optimizer, mode='min',factor=0.01, patience=2, threshold=1e-2, verbose=True)\n",
    "\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=LEARNING_RATE)\n",
    "# LR Schedular\n",
    "D_schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(D_optimizer, mode='min',factor=0.01, patience=2, threshold=1e-2, verbose=True)"
   ]
  },
  {
   "source": [
    "### Create output files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results save folder\n",
    "if not os.path.isdir('Results'):\n",
    "    os.mkdir('Results')\n",
    "if not os.path.isdir('Results/Random'):\n",
    "    os.mkdir('Results/Random')\n",
    "if not os.path.isdir('Results/Fixed'):\n",
    "    os.mkdir('Results/Fixed')"
   ]
  },
  {
   "source": [
    "### Training loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_loss(predictions):\n",
    "    batch_size = predictions.shape[0]\n",
    "    labels = torch.ones(batch_size)\n",
    "    # We use the binary cross entropy loss | Model has a sigmoid function\n",
    "    criterion = nn.BCELoss()\n",
    "    # Move models to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        labels = labels.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    loss = criterion(predictions.squeeze(), labels)\n",
    "    return loss\n",
    "\n",
    "def fake_loss(predictions):\n",
    "    batch_size = predictions.shape[0]\n",
    "    labels = torch.zeros(batch_size)\n",
    "    criterion = nn.BCELoss()\n",
    "    # Move models to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        labels = labels.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    loss = criterion(predictions.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(generator, discriminator, optimizer, real_data, batch_size, z_size):\n",
    "    # Reshape real_data to vector\n",
    "    real_data = real_data.view(batch_size, -1)\n",
    "    # Rescale real_data to range -1 - 1\n",
    "    real_data = scale(real_data)\n",
    "    \n",
    "    # Reset gradients and set model to training mode\n",
    "    optimizer.zero_grad()\n",
    "    discriminator.train()\n",
    "    \n",
    "    # Train on real data\n",
    "    real_data_logits = discriminator.forward(real_data)\n",
    "    loss_real = real_loss(real_data_logits)\n",
    "    # Generate fake data\n",
    "    z_vec = random_vector(batch_size, z_size)\n",
    "    fake_data = generator.forward(z_vec)\n",
    "    # Train on fake data\n",
    "    fake_data_logits = discriminator.forward(fake_data)\n",
    "    loss_fake = fake_loss(fake_data_logits)\n",
    "    # Calculate total loss\n",
    "    total_loss = loss_real + loss_fake\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(generator, discriminator, optimizer, batch_size, z_size):\n",
    "    # Reset gradients and set model to training mode\n",
    "    optimizer.zero_grad()\n",
    "    generator.train()\n",
    "    # Generate fake data\n",
    "    z_vec = random_vector(batch_size, z_size)\n",
    "    fake_data = generator.forward(z_vec)\n",
    "    # Train generator with output of discriminator\n",
    "    discriminator_logits = discriminator.forward(fake_data)\n",
    "    # Reverse labels\n",
    "    loss = real_loss(discriminator_logits)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_dict = {}\n",
    "loss_dict['D_loss'] = []\n",
    "loss_dict['G_loss'] = []\n",
    "for epoch in range(EPOCHS):\n",
    "    # Local losses\n",
    "    D_loss = []\n",
    "    G_loss = []\n",
    "    # Training progress bar\n",
    "    trainProgressBar = tqdm(train_loader)\n",
    "\n",
    "    for X in trainProgressBar:\n",
    "        X = X[0]\n",
    "        if torch.cuda.is_available():\n",
    "            X = X.cuda()\n",
    "\n",
    "        # Train discriminator D\n",
    "        d_loss = train_discriminator(G, D, D_optimizer, X, X.shape[0], z_size)\n",
    "        D_loss.append(d_loss)\n",
    "\n",
    "        # [==========================xxx============================]\n",
    "        \n",
    "        # Train generator G\n",
    "        g_loss = train_generator(G, D, G_optimizer, X.shape[0], z_size)\n",
    "        G_loss.append(g_loss)\n",
    "        \n",
    "        # [==========================xxx============================]\n",
    "            # Display Epoch and loss\n",
    "        display = f'Epoch {epoch + 1}  Generator loss {torch.mean(torch.FloatTensor(G_loss))} ; Descriminator loss {torch.mean(torch.FloatTensor(D_loss))}'\n",
    "        trainProgressBar.set_description(display)\n",
    "\n",
    "    # Call lr_schedular\n",
    "    G_schedular.step(G_loss[-1])\n",
    "    D_schedular.step(D_loss[-1])\n",
    "\n",
    "    # Adding loss of this epoch to the global loss dict\n",
    "    loss_dict['D_loss'].append(torch.mean(torch.FloatTensor(D_loss)))\n",
    "    loss_dict['G_loss'].append(torch.mean(torch.FloatTensor(G_loss)))\n",
    "    # Saving the results of this epoch\n",
    "    show_result(epoch + 1, save=True, path='Results/Random/' + str(epoch + 1) + '.png')\n",
    "    show_result(epoch + 1, save=True, path='Results/Fixed/' + str(epoch + 1) + '.png', fixed_noise=FIXED_NOISE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_loss(loss_dict, save=True, path='./GAN_graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for e in range(EPOCHS):\n",
    "    img_name = 'Results/Fixed/' + str(e + 1) + '.png'\n",
    "    images.append(imageio.imread(img_name))\n",
    "imageio.mimsave('Results/generation_animation.gif', images, fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}